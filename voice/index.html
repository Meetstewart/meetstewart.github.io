<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Stewart Voice Demo</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #0a0a0f 0%, #1a1a2e 100%);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: white;
        }
        .container {
            text-align: center;
            max-width: 400px;
            padding: 40px;
        }
        h1 {
            font-size: 2.5rem;
            font-weight: 300;
            margin-bottom: 10px;
            letter-spacing: 2px;
        }
        .tagline {
            font-size: 1.1rem;
            color: #888;
            margin-bottom: 40px;
        }
        #voice-btn {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
            border: none;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 30px;
            box-shadow: 0 4px 30px rgba(59, 130, 246, 0.3);
        }
        #voice-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 6px 40px rgba(59, 130, 246, 0.5);
        }
        #voice-btn.listening {
            background: linear-gradient(135deg, #22c55e 0%, #16a34a 100%);
            animation: pulse 1.5s infinite;
        }
        #voice-btn.speaking {
            background: linear-gradient(135deg, #8b5cf6 0%, #7c3aed 100%);
            animation: pulse 1.5s infinite;
        }
        #voice-btn.thinking {
            background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
            animation: spin 2s linear infinite;
        }
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
        @keyframes spin {
            0% { box-shadow: 0 0 30px rgba(245, 158, 11, 0.4); }
            50% { box-shadow: 0 0 50px rgba(245, 158, 11, 0.7); }
            100% { box-shadow: 0 0 30px rgba(245, 158, 11, 0.4); }
        }
        #voice-btn svg {
            width: 50px;
            height: 50px;
            fill: white;
        }
        #status {
            font-size: 1rem;
            color: #888;
            margin-bottom: 10px;
            min-height: 24px;
        }
        #transcript {
            font-size: 0.9rem;
            color: #aaa;
            min-height: 50px;
            max-width: 350px;
            margin: 0 auto 20px;
            font-style: italic;
        }
        #timer {
            font-size: 2rem;
            font-weight: 200;
            color: #3b82f6;
            margin-bottom: 20px;
        }
        .timer-warning {
            color: #ef4444 !important;
        }
        .note {
            font-size: 0.85rem;
            color: #666;
            margin-top: 30px;
        }
        .cta {
            margin-top: 30px;
            padding: 12px 30px;
            background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
            border: none;
            border-radius: 8px;
            color: white;
            font-size: 1rem;
            cursor: pointer;
            text-decoration: none;
            display: none;
        }
        .cta:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 20px rgba(59, 130, 246, 0.4);
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>STEWART</h1>
        <p class="tagline">Your AI Executive Assistant</p>
        
        <div id="timer">10:00</div>
        
        <button id="voice-btn">
            <svg id="mic-icon" viewBox="0 0 24 24">
                <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.91-3c-.49 0-.9.36-.98.85C16.52 14.2 14.47 16 12 16s-4.52-1.8-4.93-4.15c-.08-.49-.49-.85-.98-.85-.61 0-1.09.54-1 1.14.49 3 2.89 5.35 5.91 5.78V20c0 .55.45 1 1 1s1-.45 1-1v-2.08c3.02-.43 5.42-2.78 5.91-5.78.1-.6-.39-1.14-1-1.14z"/>
            </svg>
        </button>
        
        <div id="status">Click to talk with Stewart</div>
        <div id="transcript"></div>
        
        <p class="note">10-minute voice demo • No signup required</p>
        
        <a href="#signup" class="cta" id="signup-cta">Get Full Access →</a>
    </div>

    <script>
        // Configuration
        const GATEWAY_URL = 'https://ai.casaxai.com/chat';  // CasaXAI Gateway
        const ELEVENLABS_VOICE = 'pNInz6obpgDQGcFmaJgB';    // Adam voice
        const MAX_SECONDS = 600; // 10 minutes
        
        // State
        let isListening = false;
        let isSpeaking = false;
        let recognition = null;
        let audioContext = null;
        let remainingSeconds = MAX_SECONDS;
        let timerInterval = null;
        let conversationHistory = [];
        
        // Elements
        const btn = document.getElementById('voice-btn');
        const status = document.getElementById('status');
        const timerEl = document.getElementById('timer');
        const transcript = document.getElementById('transcript');
        const signupCta = document.getElementById('signup-cta');
        
        // Initialize Speech Recognition
        function initSpeechRecognition() {
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                status.textContent = 'Speech recognition not supported in this browser';
                return false;
            }
            
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            
            recognition.onstart = () => {
                isListening = true;
                btn.classList.add('listening');
                btn.classList.remove('speaking', 'thinking');
                status.textContent = 'Listening...';
            };
            
            recognition.onresult = (event) => {
                let interimTranscript = '';
                let finalTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                
                transcript.textContent = finalTranscript || interimTranscript;
                
                if (finalTranscript) {
                    processUserInput(finalTranscript);
                }
            };
            
            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                if (event.error !== 'no-speech') {
                    status.textContent = 'Error: ' + event.error;
                }
                stopListening();
            };
            
            recognition.onend = () => {
                isListening = false;
                if (!isSpeaking) {
                    btn.classList.remove('listening');
                }
            };
            
            return true;
        }
        
        // Start/Stop listening
        function startListening() {
            if (remainingSeconds <= 0) {
                showExpired();
                return;
            }
            if (!recognition && !initSpeechRecognition()) return;
            
            try {
                recognition.start();
                if (!timerInterval) startTimer();
            } catch (e) {
                console.error('Failed to start recognition:', e);
            }
        }
        
        function stopListening() {
            if (recognition) {
                try { recognition.stop(); } catch(e) {}
            }
            isListening = false;
            btn.classList.remove('listening');
        }
        
        // Process user input through Gateway
        async function processUserInput(userText) {
            stopListening();
            btn.classList.add('thinking');
            btn.classList.remove('listening', 'speaking');
            status.textContent = 'Thinking...';
            
            conversationHistory.push({ role: 'user', content: userText });
            
            try {
                const response = await fetch(GATEWAY_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        messages: [
                            {
                                role: 'system',
                                content: `You are Stewart, an AI executive assistant. You're helpful, professional, and concise. Keep responses brief and conversational - this is a voice interaction. You help with scheduling, emails, tasks, and general assistance. Be warm but efficient.`
                            },
                            ...conversationHistory
                        ],
                        model: 'auto'
                    })
                });
                
                const data = await response.json();
                const assistantMessage = data.choices?.[0]?.message?.content || data.response || "I'm sorry, I couldn't process that. Could you try again?";
                
                conversationHistory.push({ role: 'assistant', content: assistantMessage });
                
                await speakResponse(assistantMessage);
                
            } catch (error) {
                console.error('Gateway error:', error);
                status.textContent = 'Connection error. Click to try again.';
                btn.classList.remove('thinking');
            }
        }
        
        // Text-to-Speech with ElevenLabs
        async function speakResponse(text) {
            btn.classList.add('speaking');
            btn.classList.remove('thinking', 'listening');
            status.textContent = 'Stewart is speaking...';
            transcript.textContent = text;
            isSpeaking = true;
            
            try {
                // Use browser's built-in TTS as fallback (ElevenLabs requires API key)
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.rate = 1.0;
                utterance.pitch = 1.0;
                
                // Try to find a good voice
                const voices = speechSynthesis.getVoices();
                const preferredVoice = voices.find(v => 
                    v.name.includes('Daniel') || 
                    v.name.includes('Google UK English Male') ||
                    v.name.includes('Alex')
                ) || voices.find(v => v.lang.startsWith('en'));
                
                if (preferredVoice) utterance.voice = preferredVoice;
                
                utterance.onend = () => {
                    isSpeaking = false;
                    btn.classList.remove('speaking');
                    status.textContent = 'Click to respond...';
                    // Auto-listen after Stewart speaks
                    setTimeout(() => {
                        if (remainingSeconds > 0) startListening();
                    }, 500);
                };
                
                speechSynthesis.speak(utterance);
                
            } catch (error) {
                console.error('TTS error:', error);
                isSpeaking = false;
                btn.classList.remove('speaking');
                status.textContent = 'Click to continue...';
            }
        }
        
        // Timer
        function startTimer() {
            timerInterval = setInterval(() => {
                remainingSeconds--;
                const mins = Math.floor(remainingSeconds / 60);
                const secs = remainingSeconds % 60;
                timerEl.textContent = `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
                
                if (remainingSeconds <= 60) {
                    timerEl.classList.add('timer-warning');
                }
                
                if (remainingSeconds <= 0) {
                    showExpired();
                }
            }, 1000);
        }
        
        function showExpired() {
            clearInterval(timerInterval);
            stopListening();
            speechSynthesis.cancel();
            status.textContent = 'Demo time expired';
            transcript.textContent = 'Sign up for unlimited access to Stewart!';
            signupCta.style.display = 'inline-block';
            btn.disabled = true;
            btn.style.opacity = 0.5;
        }
        
        // Button click handler
        btn.addEventListener('click', () => {
            if (isSpeaking) {
                speechSynthesis.cancel();
                isSpeaking = false;
                btn.classList.remove('speaking');
                status.textContent = 'Click to talk...';
            } else if (isListening) {
                stopListening();
                status.textContent = 'Click to talk with Stewart';
            } else {
                startListening();
            }
        });
        
        // Load voices
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = () => speechSynthesis.getVoices();
        }
        
        // Initialize
        initSpeechRecognition();
    </script>
</body>
</html>
